# CNN S30 Ez/NonEz opt
precision    recall  f1-score   support

           0       0.84      0.86      0.85       393
           1       0.86      0.89      0.87      1114
           2       0.84      0.86      0.85       766
           3       0.57      0.63      0.60       183
           4       0.92      0.48      0.63       120
           5       0.89      0.89      0.89       114
           6       0.88      0.59      0.71        51
           7       0.91      0.90      0.90      1521

    accuracy                           0.86      4262
   macro avg       0.84      0.76      0.79      4262
weighted avg       0.86      0.86      0.86      4262

# CNN S30 Binary opt
[I 2023-10-04 16:54:20,862] Trial 21 finished with value: 0.205410435795784 and parameters: {'num_units_dense_1': 393, 'dropout_rate_1': 0.6893174856971455, 'num_units_dense_2': 159, 'dropout_rate_2': 0.4974585765201546, 'num_units_dense_3': 111}. Best is trial 21 with value: 0.205410435795784.
## S30 v2 under sampled (10000:9200)
  precision    recall  f1-score   support

           0       0.93      0.91      0.92      3009
           1       0.91      0.92      0.91      2753

    accuracy                           0.92      5762
   macro avg       0.92      0.92      0.92      5762
weighted avg       0.92      0.92      0.92      5762

## S30 v2 default (40000:9200)
				 precision    recall  f1-score   support

           0       0.96      0.96      0.96     11796
           1       0.83      0.85      0.84      2816

    accuracy                           0.94     14612
   macro avg       0.90      0.91      0.90     14612
weighted avg       0.94      0.94      0.94     14612

## S30 v2 oversampled (40000:40000)
   			    precision    recall  f1-score   support

           0       0.98      0.93      0.96     11851
           1       0.76      0.92      0.83      2762

    accuracy                           0.93     14613
   macro avg       0.87      0.93      0.89     14613
weighted avg       0.94      0.93      0.93     14613

# 1st SubClass Labels
## Params of v1 of enzyme cnn trained on S30
Epochs trained: 64
Earstop: 50

[I 2023-10-07 13:34:48,611] Trial 14 finished with value: 0.9070761203765869 and parameters: {'num_units_dense_1': 386, 'dropout_rate_1': 0.4179955867912854, 'num_units_dense_2': 187, 'dropout_rate_2': 0.28289383930137707, 'num_units_dense_3': 54}. Best is trial 14 with value: 0.9070761203765869.
## Result for v1 of enzyme cnn trained on S30
         precision    recall  f1-score   support

           0       0.72      0.72      0.72        64
           1       0.82      0.67      0.73        27
           2       0.78      0.76      0.77        41
           3       0.33      0.22      0.27         9
           4       0.43      0.38      0.40        24
           5       0.14      0.33      0.20         6
           6       0.33      0.31      0.32        13
           7       0.87      0.50      0.63        26
           8       0.45      0.56      0.50         9
           9       0.50      0.33      0.40         3
          10       0.78      0.62      0.69        29
          11       0.93      0.81      0.86        79
          12       0.00      0.00      0.00         2
          13       0.60      0.60      0.60        10
          14       0.55      0.43      0.48        14
          15       0.33      0.12      0.18         8
          16       0.75      0.50      0.60         6
          17       0.57      0.33      0.42        12
          18       0.87      0.97      0.91       144
          19       0.95      0.92      0.93       226
          20       0.92      0.85      0.88       110
          21       0.79      0.75      0.77        69
          22       0.79      0.79      0.79        19
          23       0.92      0.94      0.93       505
          24       0.85      0.69      0.76        16
          25       0.29      0.22      0.25         9
          26       0.88      0.86      0.87       310
          27       0.82      0.95      0.88       127
          28       0.91      0.89      0.90       117
          29       0.65      0.82      0.72        92
          30       0.92      0.93      0.92       136
          31       0.22      0.18      0.20        11
          32       0.53      0.68      0.59        71
          33       0.63      0.55      0.59        82
          34       0.40      0.44      0.42         9
          35       0.20      0.29      0.24         7
          36       0.86      0.60      0.71        20
          37       0.57      0.57      0.57         7
          38       0.61      0.58      0.60        24
          39       0.86      0.93      0.89        27
          40       0.47      0.54      0.50        28
          41       0.94      0.61      0.74        28
          42       0.33      0.14      0.20         7
          43       0.67      0.75      0.71         8
          44       0.97      1.00      0.98        28
          45       0.62      1.00      0.77        10
          46       0.81      0.84      0.83        57
          47       0.67      0.73      0.70        11
          48       0.84      0.86      0.85        44
          49       1.00      0.64      0.78        11
          50       1.00      0.80      0.89        10

    accuracy                           0.82      2762
   macro avg       0.66      0.62      0.63      2762
weighted avg       0.82      0.82      0.82      2762

### Dict with labels

```python
# Subclasses need at least 10 samples to be considered as specific label

ec_to_label = { 
    "1.1": 0,
    "1.2": 1, 
    "1.3": 2,
    "1.4": 3,
    "1.5": 4,
    "1.6": 5,
    "1.7": 6,
    "1.8": 7,
    "1.11": 8,
    "1.12": 9,
    "1.13": 10,
    "1.14": 11,
    "1.15": 12,
    "1.16": 13,
    "1.17": 14,
    "1.18": 15,
    "1.21": 16,
    "1.10": 17,
    "1.20": 17,
    "1.23": 17,
    "1.97": 17,
    "2.1": 18,
    "2.3": 19,
    "2.4": 20,
    "2.5": 21,
    "2.6": 22,
    "2.7": 23,
    "2.8": 24,
    "2.2": 25,
    "2.9": 25,
    "2.10": 25,
    "3.1": 26,
    "3.2": 27,
    "3.4": 28,
    "3.5": 29,
    "3.6": 30,
    "3.3": 31,
    "3.7": 31,
    "3.8": 31,
    "3.9": 31,
    "3.11": 31,
    "3.13": 31,
    "4.1": 32,
    "4.2": 33,
    "4.3": 34,
    "4.4": 35,
    "4.6": 36,
    "4.99": 37,
    "4.7": 37,
    "4.5": 37,
    "5.1": 38,
    "5.2": 39,
    "5.3": 40,
    "5.4": 41,
    "5.5": 42,
    "5.6": 43,
    "5.7": 43,
    "5.99": 43,
    "6.1": 44,
    "6.2": 45,
    "6.3": 46,
    "6.5": 47,
    "6.4": 47,
    "6.6": 47,
    "7.1": 48,
    "7.2": 49,
    "7.3": 50,
    "7.4": 50,
    "7.5": 50,
    "7.6": 50,
}


label_to_ec = {
 0: "1.1" ,
 1: "1.2" ,
 2: "1.3" ,
 3: "1.4" ,
 4: "1.5" ,
 5: "1.6" ,
 6: "1.7" ,
 7: "1.8" ,
 8: "1.11" ,
 9: "1.12" ,
 10: "1.13" ,
 11: "1.14" ,
 12: "1.15" ,
 13: "1.16" ,
 14: "1.17" ,
 15: "1.18" ,
 16: "1.21" ,
 17: "1.10_20_23_97" ,
 18: "2.1" ,
 19: "2.3" ,
 20: "2.4" ,
 21: "2.5" ,
 22: "2.6" ,
 23: "2.7" ,
 24: "2.8" ,
 25: "2.2_9_19" ,
 26: "3.1" ,
 27: "3.2" ,
 28: "3.4" ,
 29: "3.5" ,
 30: "3.6" ,
 31: "3.3_7_8_9_11_13" ,
 32: "4.1" ,
 33: "4.2" ,
 34: "4.3" ,
 35: "4.4" ,
 36: "4.6" ,
 37: "4.99_7_5" ,
 38: "5.1" ,
 39: "5.2" ,
 40: "5.3" ,
 41: "5.4" ,
 42: "5.5" ,
 43: "5.6_7_99" ,
 44: "6.1" ,
 45: "6.2" ,
 46: "6.3" ,
 47: "6.5_4_6" ,
 48: "7.1" ,
 49: "7.2" ,
 50: "7.3_4_5_6" ,
}
```

## SMOTE CNN 1
   precision    recall  f1-score   support

           0       0.73      0.82      0.77        66
           1       0.61      0.59      0.60        29
           2       0.76      0.67      0.71        33
           3       0.62      0.57      0.59        14
           4       0.79      0.52      0.62        29
           5       0.42      0.56      0.48         9
           6       0.40      0.50      0.44        12
           7       0.75      0.71      0.73        21
           8       0.89      0.62      0.73        13
           9       0.67      0.25      0.36         8
          10       0.48      0.78      0.60        18
          11       0.85      0.79      0.82        66
          12       1.00      0.88      0.93         8
          13       0.73      0.67      0.70        12
          14       0.90      0.64      0.75        14
          15       0.60      0.50      0.55         6
          16       0.33      0.17      0.22         6
          17       0.40      0.25      0.31         8
          18       0.94      0.90      0.92       164
          19       0.90      0.86      0.88       225
          20       0.83      0.87      0.85       105
          21       0.77      0.70      0.73        70
          22       0.69      0.69      0.69        13
          23       0.86      0.93      0.89       516
          24       0.75      0.41      0.53        22
          25       0.29      0.40      0.33         5
          26       0.84      0.88      0.86       341
          27       0.88      0.89      0.89       131
          28       0.94      0.83      0.89       102
          29       0.83      0.75      0.79       106
          30       0.89      0.89      0.89       124
          31       0.44      0.44      0.44         9
          32       0.57      0.61      0.59        64
          33       0.56      0.62      0.59        87
          34       0.60      0.50      0.55        12
          35       0.25      0.12      0.17         8
          36       0.83      0.71      0.77        21
          37       0.62      0.71      0.67         7
          38       0.53      0.53      0.53        17
          39       0.95      0.91      0.93        22
          40       0.73      0.55      0.63        20
          41       0.75      0.62      0.68        29
          42       0.00      0.00      0.00         1
          43       1.00      0.92      0.96        12
          44       0.96      1.00      0.98        26
          45       0.91      0.83      0.87        12
          46       0.78      0.86      0.82        58
          47       0.59      0.71      0.65        14
          48       0.81      0.90      0.85        29
          49       0.90      1.00      0.95         9
          50       0.78      0.78      0.78         9

    accuracy                           0.81      2762
   macro avg       0.70      0.66      0.67      2762
weighted avg       0.82      0.81      0.81      2762

## V3
is in presentation:
path: "/home/malte/01_Documents/projects/pbl_binary_classifier/tf_cnn_esm2/2nd_ec_class_pred/Models/CNN_S30_OPT_V3.keras"
Params: {'num_units_dense_1': 457, 'dropout_rate_1': 0.6740827127867876, 'num_units_dense_2': 149, 'dropout_rate_2': 0.1578959816342327, 'num_units_dense_3': 182}
early_stop = tf.keras.callbacks.EarlyStopping(patience=15)

### Scores
              precision    recall  f1-score   support

           0       0.68      0.76      0.72        68
           1       0.54      0.83      0.65        18
           2       0.77      0.56      0.65        36
           3       0.07      0.17      0.10         6
           4       0.69      0.53      0.60        34
           5       0.00      0.00      0.00         7
           6       0.56      0.36      0.43        14
           7       0.51      0.78      0.62        23
           8       0.89      0.57      0.70        14
           9       0.44      0.57      0.50         7
          10       0.64      0.67      0.65        21
          11       0.76      0.88      0.82        74
          12       1.00      1.00      1.00         2
          13       1.00      0.67      0.80         9
          14       0.47      0.47      0.47        15
          15       0.50      1.00      0.67         3
          16       0.67      0.29      0.40         7
          17       0.38      0.33      0.35         9
          18       0.83      0.87      0.85       151
          19       0.93      0.94      0.93       212
          20       0.87      0.84      0.85       116
          21       0.75      0.64      0.69        69
          22       0.82      0.53      0.64        17
          23       0.93      0.93      0.93       545
          24       0.69      0.64      0.67        14
          25       0.29      0.20      0.24        10
          26       0.91      0.89      0.90       303
          27       0.86      0.84      0.85       152
          28       0.84      0.89      0.86       111
          29       0.68      0.78      0.73        83
          30       0.91      0.89      0.90       128
          31       0.19      0.25      0.21        12
          32       0.57      0.59      0.58        70
          33       0.53      0.73      0.61        81
          34       0.40      0.24      0.30        17
          35       0.00      0.00      0.00         9
          36       0.78      0.90      0.84        20
          37       1.00      0.50      0.67         6
          38       0.42      0.28      0.33        18
          39       1.00      0.97      0.98        32
          40       0.57      0.35      0.43        23
          41       0.94      0.62      0.75        24
          42       0.75      0.50      0.60         6
          43       0.47      1.00      0.64         7
          44       0.92      0.96      0.94        25
          45       0.88      0.78      0.82         9
          46       0.80      0.77      0.78        56
          47       0.75      0.75      0.75        20
          48       0.88      0.64      0.74        33
          49       0.70      0.70      0.70        10
          50       0.83      0.83      0.83         6

    accuracy                           0.81      2762
   macro avg       0.67      0.64      0.64      2762
weighted avg       0.81      0.81      0.81      2762

## V4
		  precision    recall  f1-score   support

           0       0.72      0.74      0.73        68
           1       0.64      0.78      0.70        18
           2       0.57      0.72      0.63        36
           3       0.09      0.17      0.12         6
           4       0.68      0.38      0.49        34
           5       0.40      0.29      0.33         7
           6       1.00      0.14      0.25        14
           7       0.42      0.74      0.54        23
           8       0.88      0.50      0.64        14
           9       1.00      0.43      0.60         7
          10       0.73      0.76      0.74        21
          11       0.79      0.86      0.83        74
          12       0.67      1.00      0.80         2
          13       0.78      0.78      0.78         9
          14       0.73      0.53      0.62        15
          15       0.22      0.67      0.33         3
          16       0.20      0.14      0.17         7
          17       0.67      0.22      0.33         9
          18       0.80      0.90      0.85       151
          19       0.95      0.92      0.93       212
          20       0.84      0.84      0.84       116
          21       0.70      0.67      0.68        69
          22       0.92      0.65      0.76        17
          23       0.92      0.92      0.92       545
          24       0.58      0.50      0.54        14
          25       0.33      0.10      0.15        10
          26       0.87      0.90      0.88       303
          27       0.84      0.87      0.85       152
          28       0.86      0.88      0.87       111
          29       0.62      0.77      0.69        83
          30       0.85      0.91      0.88       128
          31       0.40      0.17      0.24        12
          32       0.53      0.53      0.53        70
          33       0.53      0.62      0.57        81
          34       0.33      0.12      0.17        17
          35       0.00      0.00      0.00         9
          36       0.62      0.80      0.70        20
          37       0.60      0.50      0.55         6
          38       0.38      0.17      0.23        18
          39       0.97      0.94      0.95        32
          40       0.60      0.39      0.47        23
          41       0.79      0.79      0.79        24
          42       0.00      0.00      0.00         6
          43       0.57      0.57      0.57         7
          44       1.00      0.96      0.98        25
          45       0.86      0.67      0.75         9
          46       0.73      0.80      0.76        56
          47       0.81      0.65      0.72        20
          48       0.83      0.73      0.77        33
          49       0.67      0.60      0.63        10
          50       0.83      0.83      0.83         6

    accuracy                           0.80      2762
   macro avg       0.65      0.60      0.60      2762
weighted avg       0.80      0.80      0.79      2762


## Weighted v1

same params as v3
              precision    recall  f1-score   support

           0       0.68      0.74      0.71        68
           1       0.78      0.78      0.78        18
           2       0.66      0.75      0.70        36
           3       0.13      0.33      0.19         6
           4       0.60      0.53      0.56        34
           5       0.50      0.71      0.59         7
           6       0.75      0.43      0.55        14
           7       0.67      0.61      0.64        23
           8       0.69      0.64      0.67        14
           9       0.36      0.57      0.44         7
          10       0.60      0.71      0.65        21
          11       0.84      0.82      0.83        74
          12       0.67      1.00      0.80         2
          13       1.00      0.78      0.88         9
          14       0.89      0.53      0.67        15
          15       0.29      0.67      0.40         3
          16       0.33      0.29      0.31         7
          17       0.33      0.44      0.38         9
          18       0.88      0.74      0.80       151
          19       0.90      0.81      0.85       212
          20       0.69      0.78      0.73       116
          21       0.77      0.54      0.63        69
          22       0.87      0.76      0.81        17
          23       0.96      0.70      0.81       545
          24       0.45      0.64      0.53        14
          25       0.46      0.60      0.52        10
          26       0.65      0.73      0.69       303
          27       0.78      0.82      0.80       152
          28       0.62      0.81      0.70       111
          29       0.41      0.73      0.53        83
          30       0.80      0.87      0.83       128
          31       0.24      0.42      0.30        12
          32       0.39      0.57      0.46        70
          33       0.42      0.54      0.47        81
          34       0.80      0.24      0.36        17
          35       0.00      0.00      0.00         9
          36       0.84      0.80      0.82        20
          37       1.00      0.50      0.67         6
          38       0.33      0.39      0.36        18
          39       1.00      0.97      0.98        32
          40       0.78      0.30      0.44        23
          41       0.79      0.79      0.79        24
          42       0.75      0.50      0.60         6
          43       0.86      0.86      0.86         7
          44       1.00      0.96      0.98        25
          45       0.58      0.78      0.67         9
          46       0.77      0.84      0.80        56
          47       0.74      0.70      0.72        20
          48       0.95      0.61      0.74        33
          49       0.58      0.70      0.64        10
          50       0.83      0.83      0.83         6

    accuracy                           0.72      2762
   macro avg       0.66      0.65      0.64      2762
weighted avg       0.76      0.72      0.73      2762


# Params for each FNN:
## Level 0
best_units = {'num_units_dense_1': 202, 'dropout_rate_1': 0.06940982765055399, 'num_units_dense_2': 35}

## Level 1
best_params = {'num_units_dense_1': 168, 'dropout_rate_1': 0.07889805610819639, 'num_units_dense_2': 43}

## Level 2
best_params = {'num_units_dense_1': 262, 'dropout_rate_1': 0.06908699888382129, 'num_units_dense_2': 121}
